{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b9454a",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d8056",
   "metadata": {},
   "source": [
    "## Load enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e38c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b8e13",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380cafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from dataclasses import dataclass\n",
    "from retry import retry\n",
    "from typing import Union, Literal, Optional, Generator\n",
    "import json\n",
    "from IPython.display import Markdown, clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f498fb",
   "metadata": {},
   "source": [
    "## List models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc71b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models\n",
      "1: gemma2:2b\n",
      "2: gemma2:latest\n",
      "3: all-minilm:latest\n",
      "4: qwen2:7b-instruct-q6_K\n",
      "5: llama3.1:latest\n",
      "6: llava:13b\n",
      "7: phi:latest\n",
      "8: gemma:7b-instruct-q6_K\n",
      "9: mxbai-embed-large:latest\n"
     ]
    }
   ],
   "source": [
    "print(\"Available models\")\n",
    "for i, model in enumerate(ollama.list()[\"models\"], 1):\n",
    "    print(f\"{i}: {model['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb31758",
   "metadata": {},
   "source": [
    "## Configable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec6cd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemma2:2b\"\n",
    "# MODEL = \"gemma2:latest\"\n",
    "# MODEL = \"phi:latest\"\n",
    "EMBEDDING_MODEL = \"all-minilm:latest\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "RETRY_COUNT = 5\n",
    "SELECT_TOP_RESULTS = 3\n",
    "INDEX_NAME = \"sfc_code_preprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fc4aa",
   "metadata": {},
   "source": [
    "# OLLAMA client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ca3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client: ollama.Client = ollama.Client(host=OLLAMA_HOST)\n",
    "\n",
    "def get_embedding(text: str, embedding_model: str) -> list[float]:\n",
    "    response = ollama_client.embeddings(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        prompt=text,\n",
    "    )\n",
    "    return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb5ce1",
   "metadata": {},
   "source": [
    "# Opensearch client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d695f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSEARCH_USERNAME = os.environ[\"OPENSEARCH_USERNAME\"]\n",
    "OPENSEARCH_PASSWORD = os.environ[\"OPENSEARCH_PASSWORD\"]\n",
    "OPENSEARCH_URL = os.environ[\"OPENSEARCH_URL\"]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def get_open_search(cluster_url: str, username: str, password: str):\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url], http_auth=(username, password), verify_certs=False\n",
    "    )\n",
    "    return client\n",
    "\n",
    "open_search_client: OpenSearch = get_open_search(\n",
    "    OPENSEARCH_URL, OPENSEARCH_USERNAME, OPENSEARCH_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ac751",
   "metadata": {},
   "source": [
    "# Get distinct topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675e9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_search(cluster_url: str, username: str, password: str):\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url], http_auth=(username, password), verify_certs=False\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "open_search_client: OpenSearch = get_open_search(\n",
    "    OPENSEARCH_URL, OPENSEARCH_USERNAME, OPENSEARCH_PASSWORD\n",
    ")\n",
    "\n",
    "\n",
    "results = open_search_client.search(\n",
    "    body={\n",
    "        \"size\": 0,\n",
    "        \"aggs\": {\n",
    "            \"distinct_sources\": {\n",
    "                \"composite\": {\n",
    "                    \"sources\": [\n",
    "                        {\"topic_title\": {\"terms\": {\"field\": \"topic_title.keyword\"}}},\n",
    "                        {\"file_url\": {\"terms\": {\"field\": \"file_url.keyword\"}}},\n",
    "                    ],\n",
    "                    \"size\": 10000,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    index=INDEX_NAME,\n",
    ")\n",
    "\n",
    "buckets = results[\"aggregations\"][\"distinct_sources\"][\"buckets\"]\n",
    "buckets_topic_to_url = {\n",
    "    bucket[\"key\"][\"topic_title\"]: bucket[\"key\"][\"file_url\"] for bucket in buckets\n",
    "}\n",
    "topic_list = list(buckets_topic_to_url.keys())\n",
    "topic_choices: str = \"\\n\".join([f\"{i}. {topic}\" for i, topic in enumerate(topic_list, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12527601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Code of Conduct for Persons Providing Credit Rating Services\n",
      "2. Code of Conduct for Share Registrars\n",
      "3. Code on Immigration-Linked Investment Schemes\n",
      "4. Code on Open-ended Fund Companies\n",
      "5. Code on Pooled Retirement Funds\n",
      "6. Code on Real Estate Investment Trusts\n",
      "7. Corporate Finance Adviser Code of Conduct\n",
      "8. Fund Manager Code of Conduct\n",
      "9. SFC Code on MPF Products\n",
      "10. Section I - Overarching Principles Section\n",
      "11. Section II - Code on Unit Trusts and Mutual Funds\n",
      "12. Section III - Code on Investment-Linked Assurance Schemes\n",
      "13. Section IV - Code on Unlisted Structured Investment Products\n",
      "14. The Codes on Takeovers and Mergers and Share Buy-backs\n"
     ]
    }
   ],
   "source": [
    "print(topic_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cdca1",
   "metadata": {},
   "source": [
    "# Prompt handlers\n",
    "## Topic selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e59a7d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TopicSelector:\n",
    "    verbose: int = 0\n",
    "    header: str = (\n",
    "        \"Pick an index of document that you think that it can help answer the following question or pick 0 if you think they are not helpful. Please answer only as a number and do not include prologue, prefix or suffix\"\n",
    "    )\n",
    "    system_prompt: str = (\n",
    "        \"Pick a choice, please answer only a number and do not include prologue, prefix or suffix\"\n",
    "    )\n",
    "    topic_choices: tuple = tuple(topic_list)\n",
    "\n",
    "    def generate(self, prompt: str) -> int:\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        try:\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk[\"message\"][\"content\"]\n",
    "\n",
    "        finally:\n",
    "            stream.close()\n",
    "        return response\n",
    "\n",
    "    def construct_prompt(self, question: str) -> str:\n",
    "        header = self.header\n",
    "        topic_choices = \"\\n\".join(\n",
    "            [f\"{i}. {topic}\" for i, topic in enumerate(self.topic_choices, 1)]\n",
    "        )\n",
    "        prompt = (\n",
    "            topic_selection_prompt\n",
    "        ) = f\"\"\"{header}\n",
    "\n",
    "# Available source:\n",
    "{topic_choices}\n",
    "\n",
    "# question:\n",
    "{question}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @retry(tries=RETRY_COUNT, exceptions=ValueError)\n",
    "    def pick_a_choice(self, question) -> int:\n",
    "        prompt = self.construct_prompt(question)\n",
    "        result = int(self.generate(prompt))\n",
    "        assert result >= 0, \"Invalid generated result, Regenerating...\"\n",
    "        return int(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9216aeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicSelector().pick_a_choice(\"Tell me about real estate law\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c24f8a",
   "metadata": {},
   "source": [
    "## Query builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1464832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query builder\n",
    "\n",
    "@dataclass\n",
    "class TextQueryBuilder:\n",
    "    verbose: int = 0\n",
    "    header: str = (\n",
    "        \"Based on the following question, what keywords should be queried in Opensearch\"\n",
    "    )\n",
    "    system_prompt: str = (\n",
    "        \"We have an Opensearch instant storing docuements about code of conduct.\"\n",
    "        \" You are a data engineer who expertise Opensearch query.\"\n",
    "        \" Please suggest text query based on user's question\"\n",
    "        \" return your answer only  and do not include prologue, prefix or suffix\"\n",
    "    )\n",
    "    topic_choices: tuple = tuple(topic_list)\n",
    "\n",
    "    def generate(self, prompt: str) -> int:\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        try:\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk[\"message\"][\"content\"]\n",
    "        finally:\n",
    "            stream.close()\n",
    "        return response\n",
    "\n",
    "    def construct_prompt(self, question: str) -> str:\n",
    "        header = self.header\n",
    "        prompt = (\n",
    "            topic_selection_prompt\n",
    "        ) = f\"\"\"{header}\n",
    "\n",
    "# question:\n",
    "{question}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @retry(tries=RETRY_COUNT)\n",
    "    def build(self, question) -> int:\n",
    "        prompt = self.construct_prompt(question)\n",
    "        result = self.generate(prompt)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_topic(question: str, verbose: int = 0) -> str:\n",
    "    topic_selected_index = topic_selector.pick_a_choice(question)\n",
    "    if topic_selected_index:\n",
    "        selected_topic = topic_selector.topic_choices[topic_selected_index - 1]\n",
    "        if verbose:\n",
    "            print(\n",
    "                f'THE QUESTION: \"{question}\" \\nSELECTED TOPIC: {topic_selected_index}. \"{selected_topic}\"\\nFROM {buckets_topic_to_url[selected_topic]}'\n",
    "            )\n",
    "        return selected_topic\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Provided sources are not seem related to the question\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f9390",
   "metadata": {},
   "source": [
    "## Source summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82704537",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SourceSummarizer:\n",
    "    system_prompt: str = (\n",
    "    \"You are an expert in lawfirm who are assigned to consider whether a text data source \"\n",
    "    \"is useful to answer a user question or not. If yes, you will summarize the text \"\n",
    "    \"which corespond user's question for another expert to write answer the user , otherwise, do nothing. \"\n",
    "    '''You answer must be in JSON format with field:\n",
    "\"is_useful\": boolean determining whether the source is useful,\n",
    "\"summarize: string your summarization refering the part for the text or empty string if not useful\n",
    "    '''\n",
    "     \" return your answer only and do not include prologue, prefix or suffix\"\n",
    "    )\n",
    "    def generate(self, prompt: str) -> int:\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        try:\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk[\"message\"][\"content\"]\n",
    "        finally:\n",
    "            stream.close()\n",
    "        return response\n",
    "    \n",
    "    def construct_prompt(self, question: str, text_source: str) -> str:\n",
    "        prompt = (\n",
    "            topic_selection_prompt\n",
    "        ) = f\"\"\"# Source:\n",
    "{text_source}\n",
    "# question:\n",
    "{question}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @retry(tries=RETRY_COUNT, exceptions=json.JSONDecodeError)\n",
    "    def summarize(self, question: str, text_source: str):\n",
    "        prompt = self.construct_prompt(question, text_source)\n",
    "        result = self.generate(prompt)\n",
    "        print(result)\n",
    "        result = result.strip().strip(\"`\").lstrip(\"json\")\n",
    "        print(result)\n",
    "        return json.loads(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7658b3",
   "metadata": {},
   "source": [
    "## User interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c20a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UserInteractor:\n",
    "    system_prompt: str = (\n",
    "        \"You are an humble expert in lawfirm, and your secretary already \"\n",
    "        \"prepared gists from the related document for \"\n",
    "        \"you to answer user's question. \"\n",
    "        \"Your duty is to answer the question \"\n",
    "        \"with confidence using the prepared \"\n",
    "        \"data source as a reference.\"\n",
    "        \"Please also add the reference of data source with URL to PDF file with page number \"\n",
    "        \"and encourage user to find out more information with it\"\n",
    "    )\n",
    "\n",
    "    def generate(self, prompt: str, stream: bool = False) -> str:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=stream,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def stream_text(\n",
    "        self,\n",
    "        generator: Generator[None, int, None],\n",
    "        additional_text: str = \"\",\n",
    "    ) -> Generator[None, int, None]:\n",
    "        for chunk in generator:\n",
    "            yield chunk[\"message\"][\"content\"]\n",
    "        yield from additional_text\n",
    "\n",
    "    def construct_prompt(\n",
    "        self, question: str, topic: str, contexts: list[str], source_url: str\n",
    "    ) -> str:\n",
    "        system_prompt = self.system_prompt.format()\n",
    "        context_prompt = \"- \" + \"\\n- \".join(contexts)\n",
    "\n",
    "        prompt = (\n",
    "            topic_selection_prompt\n",
    "        ) = f\"\"\"# question:\n",
    "{question}\n",
    "\n",
    "# Prepared data source:\n",
    "Document: {topic}\n",
    "{context_prompt}\n",
    "URL: {source_url}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def answer(\n",
    "        self, question: str, topic: str, contexts: list[str], stream: bool = False\n",
    "    ) -> str:\n",
    "        source_url = buckets_topic_to_url[topic]\n",
    "        prompt = self.construct_prompt(question, topic, contexts, source_url)\n",
    "        response = self.generate(prompt, stream)\n",
    "        references = (\n",
    "            \"<br><br>**Reference**\\n\"\n",
    "            f\"> From: {source_url}\"\n",
    "            \"\\n* \"\n",
    "            + \"\\n* \".join(sorted(contexts, key=lambda x: int(x.rsplit(\" \", 1)[1])))\n",
    "        )\n",
    "        if not stream:\n",
    "            return response[\"message\"][\"content\"] + references\n",
    "\n",
    "        return self.stream_text(response, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7149",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e77c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data_in_opensearch(\n",
    "    query: str,\n",
    "    search_method: Union[Literal[\"text\"], Literal[\"vector\"]],\n",
    "    topic_title: Optional[str],\n",
    ") -> dict:\n",
    "    \n",
    "    query_embedding = get_embedding(question, EMBEDDING_MODEL)\n",
    "\n",
    "    if search_method == \"vector\":\n",
    "        must = [{\"knn\": {\"embedding\": {\"vector\": query_embedding, \"k\": 5}}}]\n",
    "    elif search_method == \"text\":\n",
    "        must = [\n",
    "            {\n",
    "                \"match\": {\n",
    "                    \"text\": {\n",
    "                        \"query\": query,\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid search method\")\n",
    "    must += [\n",
    "        {\n",
    "            \"match\": {\n",
    "                \"topic_title\": {\n",
    "                    \"query\": topic_title,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    query_body = {\n",
    "        \"query\": {\"bool\": {\"must\": must}},\n",
    "        \"_source\": False,\n",
    "        \"fields\": [\"id\", \"topic_title\", \"text\", \"file_url\", \"page_number\"],\n",
    "    }\n",
    "\n",
    "    results = open_search_client.search(body=query_body, index=INDEX_NAME)\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_search_results(search_results_raw: dict) -> list[str]:\n",
    "    return [\n",
    "        {\n",
    "            \"text\": result[\"fields\"][\"text\"][0],\n",
    "            \"topic\": result[\"fields\"][\"topic_title\"][0],\n",
    "            \"url\": result[\"fields\"][\"file_url\"][0],\n",
    "            \"page\": result[\"fields\"][\"page_number\"][0],\n",
    "        }\n",
    "        for result in search_results_raw[\"hits\"][\"hits\"][:SELECT_TOP_RESULTS]\n",
    "    ]\n",
    "\n",
    "\n",
    "def summarize_into_contexts(\n",
    "    source_summarizer: SourceSummarizer, search_results: list\n",
    ") -> list[str]:\n",
    "    contexts = []\n",
    "    for search_result in search_results:\n",
    "        summarized = source_summarizer.summarize(question, search_result[\"text\"])\n",
    "        topic = search_result[\"topic\"]\n",
    "        page = search_result[\"page\"]\n",
    "        if summarized[\"is_useful\"]:\n",
    "            summarized_text = summarized[\"summarize\"]\n",
    "            context = f'\"{summarized_text}\" - Page: {page}'\n",
    "            contexts.append(context)\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def apologize(question: str, stream: bool = False) -> str:\n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Apologize the inqueriing user \"\n",
    "                    \"because we're don't have any information or duty to \"\n",
    "                    \"answer user's question.\"\n",
    "                    \"If possible, suggest any another helpful resource that \"\n",
    "                    \"he may find the answer\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f'Let the user know that you cannot the question \"{question}\" due to you don\\'t have information which user just enqueried.'},\n",
    "        ],\n",
    "        stream=stream,\n",
    "    )\n",
    "    if not stream:\n",
    "        return response[\"message\"][\"content\"]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92721f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a788306",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_selector = TopicSelector()\n",
    "text_query_builder = TextQueryBuilder()\n",
    "source_summarizer = SourceSummarizer()\n",
    "user_interactor = UserInteractor()\n",
    "\n",
    "\n",
    "\n",
    "def answer_the_question(question: str, stream: bool = False, debug: bool = False) -> Union[Generator[str, None, None], str]:\n",
    "    \"\"\"Run the responding flow to answer user's question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): User's question\n",
    "        stream (bool): Choose how to emitting the answer\n",
    "    Returns:\n",
    "        - Generator[str, None, None] if stream = True\n",
    "        - str if stream = False\n",
    "    \"\"\"\n",
    "    topic_selected_index = topic_selector.pick_a_choice(question)\n",
    "    if topic_selected_index < 1:\n",
    "        # We don't have information to answer the question.\n",
    "        return apologize(question, stream)\n",
    "    topic_title = topic_selector.topic_choices[topic_selected_index - 1]\n",
    "    if debug: print(f\"{topic_title=}\")\n",
    "    query_text = text_query_builder.build(question)\n",
    "    if debug: print(f\"{query_text=}\")\n",
    "    # Query\n",
    "    search_text_results_raw = search_data_in_opensearch(\n",
    "        query_text, search_method=\"text\", topic_title=topic_title\n",
    "    )\n",
    "    search_vector_results_raw = search_data_in_opensearch(\n",
    "        query_text, search_method=\"vector\", topic_title=topic_title\n",
    "    )\n",
    "    search_text_results = extract_search_results(search_text_results_raw)\n",
    "    search_vector_results = extract_search_results(search_vector_results_raw)\n",
    "\n",
    "    # Summerize useful resources\n",
    "    if debug: print(\"summarizing contexts\")\n",
    "    context_text_results = summarize_into_contexts(source_summarizer, search_text_results)\n",
    "    if debug: print(f\"{context_text_results=}\")\n",
    "    context_vector_results = summarize_into_contexts(\n",
    "        source_summarizer, search_vector_results\n",
    "    )\n",
    "    contexts = context_text_results + context_vector_results\n",
    "    if len(contexts) == 0:\n",
    "        # The retrieved resources are not useful. \n",
    "        return apologize(question, stream)\n",
    "    # Generate the answer\n",
    "    if debug: print(\"start generate the answer\")\n",
    "    answer = user_interactor.answer(question, topic=topic_title, contexts=contexts, stream=stream)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def display_answer(answer: Union[Generator[str, None, None], str]):\n",
    "    if isinstance(answer, Generator):\n",
    "        cumulative_response = \"\"\n",
    "        for c in answer:\n",
    "            if isinstance(c, dict):\n",
    "                c = c[\"message\"][\"content\"]\n",
    "            print(c, end=\"\", flush=True)\n",
    "            cumulative_response += c\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(cumulative_response))\n",
    "    else:\n",
    "        display(Markdown(answer))\n",
    "        \n",
    "        \n",
    "# Display output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf1254",
   "metadata": {},
   "source": [
    "# Example\n",
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b84abf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It's great you're interested in real estate investment!  A key detail to know is that successful real estate investments often focus on generating recurring income.\n",
       "\n",
       "According to the Code on Real Estate Investment Trusts, at least 75% of a scheme's assets **must** be invested in properties that produce consistent rental income. This ensures a steady flow of cash for investors.  You can read more about this requirement on page 35 and 37 of the document.\n",
       "\n",
       "It's also crucial to understand the specific types of real estate being considered by the investment scheme. The offering document, which you can find at [https://www.sfc.hk/-/media/EN/files/COM/Reports-and-surveys/REIT-Code_Aug2022_en.pdf?rev=572cff969fc344fe8c375bcaab427f3b](https://www.sfc.hk/-/media/EN/files/COM/Reports-and-surveys/REIT-Code_Aug2022_en.pdf?rev=572cff969fc344fe8c375bcaab427f3b), details the investment strategy, including the types of properties (residential, commercial, industrial) and the market conditions they operate in (page 78).\n",
       "\n",
       "I encourage you to thoroughly review the offering document to make an informed decision.\n",
       "\n",
       "\n",
       "<br><br>**Reference**\n",
       "> From: https://www.sfc.hk/-/media/EN/files/COM/Reports-and-surveys/REIT-Code_Aug2022_en.pdf?rev=572cff969fc344fe8c375bcaab427f3b\n",
       "* \"The scheme primarily invests in income-generating real estate. At least 75% of the gross asset value must be invested in real estate generating recurrent rental income. The scheme may acquire up to 25% of its gross asset value in uncompleted units subject to certain conditions and disclosures.\" - Page: 35\n",
       "* \"At least 75% of a scheme's gross asset value shall be invested in real estate that generates recurrent rental income at all times.\" - Page: 37\n",
       "* \"The offering document of the scheme shall clearly include: (c) a discussion of the business plan for property investment and management covering the scope and type of investments made or intended to be made by the scheme, including the type(s) of real estate (e.g. residential/commercial/industrial);  (d) the general character and competitive conditions of all real estate now held or intended to be acquired by the scheme and how it\" - Page: 78"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question: str = \"I want to invest in real estates. What detail should I know?\"\n",
    "answer = answer_the_question(question, stream=True, debug=True)\n",
    "display_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a95459",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1f443f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a product about pooled retirement funds is an exciting venture!  \n",
       "\n",
       "Here are some suggestions based on the \"Code on Pooled Retirement Funds\":\n",
       "\n",
       "* **Focus on transparency:** The code emphasizes clear and comprehensive information for investors. Your product should have a well-structured \"principal brochure\" (offering document) that outlines fees, investment strategies, termination conditions, and transfer/withdrawal rules clearly and accessibly.  Think about using visuals and simple language to make complex financial information understandable.\n",
       "* **Tailor your offering:** Consider different investor needs and risk appetites. You could offer various PRF options with diverse investment strategies – some conservative, others more aggressive.  \n",
       "\n",
       "Remember, the \"Code on Pooled Retirement Funds\" is a valuable resource for developing your product. It provides detailed guidance on regulations and best practices. \n",
       "\n",
       "\n",
       "For more in-depth information, I encourage you to explore the document yourself: [https://www.sfc.hk/-/media/EN/assets/components/codes/files-current/web/codes/code-on-pooled-retirement-funds/code-on-pooled-retirement-funds.pdf?rev=9badf81950734ee08c799832be6ff92b](https://www.sfc.hk/-/media/EN/assets/components/codes/files-current/web/codes/code-on-pooled-retirement-funds/code-on-pooled-retirement-funds.pdf?rev=9badf81950734ee08c799832be6ff92b) (Page 12, 45).  \n",
       "\n",
       "\n",
       "Good luck with your product development!<br><br>**Reference**\n",
       "> From: https://www.sfc.hk/-/media/EN/assets/components/codes/files-current/web/codes/code-on-pooled-retirement-funds/code-on-pooled-retirement-funds.pdf?rev=9badf81950734ee08c799832be6ff92b\n",
       "* \"This text defines several terms related to Pooled Retirement Funds (PRFs) such as 'principal brochure', 'Product Code', 'Product Provider' and refers to relevant legislation like Occupational Retirement Schemes Ordinance. It outlines the process of having PRFs authorized by the Commission.\" - Page: 12\n",
       "* \"  3.10 “pooled retirement fund” or “PRF” has the same meaning as “pooling agreement” in the Occupational Retirement Schemes Ordinance (Chapter 426 of Laws of Hong Kong). \n",
       "\n",
       "3.11 “principal brochure” or “offering document” means documents (including any other documents issued together) containing information on a PRF and investment portfolio(s) as stipulated in Appendix A. \n",
       "\n",
       "3.11A “Product Code” means any of the following codes administered by the Commission: \n",
       " (a) Code on Unit Trusts and Mutual Funds \n",
       "(b) Code on Investment-Linked Assurance Schemes \n",
       "(c) Code on Pooled Retirement Funds \n",
       "(d) SFC Code on MPF Products\" - Page: 12\n",
       "* \"The text describes various aspects of a Pooled Retirement Fund (PRF) including fees, investment strategy, termination conditions, and transfer/withdrawal rules. This information could be helpful in designing a product about pooled retirement funds.\" - Page: 45"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question: str = \"I want to design product about pooled Retirement funds, any suggestion?\"\n",
    "answer = answer_the_question(question, stream=True)\n",
    "display_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05dfa3",
   "metadata": {},
   "source": [
    "## Example 3 (Unrelated question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40d1040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but I'm afraid I won't be able to provide an answer to your question on how to cook fried chicken as we don't have any relevant information or duty to answer your query.\n",
       "\n",
       "However, if you're looking for a delicious recipe, there are many other helpful resources available online that can guide you through the process. You might want to check out reputable cooking websites such as Food.com, Allrecipes, or Epicurious, which offer a wide range of recipes and tutorials on how to cook fried chicken.\n",
       "\n",
       "Additionally, you could also consider searching for videos on YouTube or Pinterest, where many home cooks and chefs share their favorite recipes and cooking techniques. You might find some inspiration and guidance from there!\n",
       "\n",
       "If you have any more specific questions or need further assistance in finding the information you're looking for, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question: str = \"How to cook fried chicken?\"\n",
    "answer = answer_the_question(question, stream=True)\n",
    "display_answer(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
